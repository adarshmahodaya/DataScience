{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark DataFrame and Sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: string, Plate ID: string, Registration State: string, Issue Date: string, Violation Code: string, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: string, Issuer Precinct: string, Violation Time: string]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|         0852P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03|            40|              SDN|       TOYOT|                71|             71|         0215A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21|            36|               UT|         BMW|                 0|              0|         0758A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21|            36|             SUBN|       DODGE|                 0|              0|         1005A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05|             5|               4D|         BMW|                 0|              0|         0845A|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|         0015A|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27|            19|             DELV|       KENWO|                18|             18|         0707A|\n",
      "|    4625926610|N102911C|                NY|2016-10-27|            36|              VAN|        FORD|                 0|              0|         1022A|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30|            21|              SDN|       NISSA|                44|             44|         1150A|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|         0525A|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07|            71|             4DSD|       VOLKS|               120|            120|         0645P|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24|             7|               TK|        FORD|                 0|              0|         1122A|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|         0256P|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30|            20|             SUBN|       DODGE|                17|             17|         1232A|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03|            36|               4D|         BMW|                 0|              0|         1034A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: string (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Code: string (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: string (nullable = true)\n",
      " |-- Issuer Precinct: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10803028, 10)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10803028\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "print(df.select(col('Summons Number')).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Total number of parking tickets :  10803028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Registration State|\n",
      "+------------------+\n",
      "|                AZ|\n",
      "|                SC|\n",
      "|                NS|\n",
      "|                LA|\n",
      "|                MN|\n",
      "|                NJ|\n",
      "|                MX|\n",
      "|                DC|\n",
      "|                OR|\n",
      "|                99|\n",
      "|                NT|\n",
      "|                VA|\n",
      "|                RI|\n",
      "|                KY|\n",
      "|                WY|\n",
      "|                BC|\n",
      "|                NH|\n",
      "|                MI|\n",
      "|                GV|\n",
      "|                NV|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Registration State')).distinct().show()\n",
    "print(df.select(col('Registration State')).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration State '99' seems to be erronous data.\n",
    "### Need to replace by state having maximum entries.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_RegistrationState = df.groupby('Registration State').agg({'Summons Number' : 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|Registration State|count(Summons Number)|\n",
      "+------------------+---------------------+\n",
      "|                NY|              8481061|\n",
      "|                NJ|               925965|\n",
      "|                PA|               285419|\n",
      "|                FL|               144556|\n",
      "|                CT|               141088|\n",
      "|                MA|                85547|\n",
      "|                IN|                80749|\n",
      "|                VA|                72626|\n",
      "|                MD|                61800|\n",
      "|                NC|                55806|\n",
      "|                IL|                37329|\n",
      "|                GA|                36852|\n",
      "|                99|                36625|\n",
      "|                TX|                36516|\n",
      "|                AZ|                26426|\n",
      "|                OH|                25302|\n",
      "|                CA|                24260|\n",
      "|                SC|                21836|\n",
      "|                ME|                21574|\n",
      "|                MN|                18227|\n",
      "+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grouped_RegistrationState = df_grouped_RegistrationState.sort(['count(Summons Number)'], ascending = False)\n",
    "df_grouped_RegistrationState.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Registration State '99' by 'NY'(maximum in terms of tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('Registration State',\n",
    "    F.when(df['Registration State']=='99','NY').\n",
    "    otherwise(df['Registration State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|Registration State|count(Summons Number)|\n",
      "+------------------+---------------------+\n",
      "|                NY|              8517686|\n",
      "|                NJ|               925965|\n",
      "|                PA|               285419|\n",
      "|                FL|               144556|\n",
      "|                CT|               141088|\n",
      "|                MA|                85547|\n",
      "|                IN|                80749|\n",
      "|                VA|                72626|\n",
      "|                MD|                61800|\n",
      "|                NC|                55806|\n",
      "|                IL|                37329|\n",
      "|                GA|                36852|\n",
      "|                TX|                36516|\n",
      "|                AZ|                26426|\n",
      "|                OH|                25302|\n",
      "|                CA|                24260|\n",
      "|                SC|                21836|\n",
      "|                ME|                21574|\n",
      "|                MN|                18227|\n",
      "|                OK|                18165|\n",
      "+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grouped_RegistrationState = df.groupby('Registration State').agg({'Summons Number' : 'count'})\n",
    "df_grouped_RegistrationState = df_grouped_RegistrationState.sort(['count(Summons Number)'], ascending = False)\n",
    "df_grouped_RegistrationState.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '99' is now replaced by 'NY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(df.select(col('Registration State')).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Total number of unique states from where the cars that got parking tickets came : 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------------------------  ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|Violation Code|count(Summons Number)|\n",
      "+--------------+---------------------+\n",
      "|             7|               516395|\n",
      "|            51|                61389|\n",
      "|            54|                   11|\n",
      "|            15|                   22|\n",
      "|            11|                 7127|\n",
      "+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. frequency of the top five violation codes.\n",
    "\n",
    "df_grouped_ViolationCode = df.groupby(\"Violation Code\").agg({'Summons Number' : 'count'})\n",
    "df_grouped_ViolationCode.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Violation code are :\n",
      "+--------------+---------------------+\n",
      "|Violation Code|count(Summons Number)|\n",
      "+--------------+---------------------+\n",
      "|            21|              1528588|\n",
      "|            36|              1400614|\n",
      "|            38|              1062304|\n",
      "|            14|               893498|\n",
      "|            20|               618593|\n",
      "+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 Violation code are :')\n",
    "df_grouped_ViolationCode.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 vehicle body type that gets ticket are :\n",
      "+-----------------+---------------------+\n",
      "|Vehicle Body Type|count(Summons Number)|\n",
      "+-----------------+---------------------+\n",
      "|             SUBN|              3719802|\n",
      "|             4DSD|              3082020|\n",
      "|              VAN|              1411970|\n",
      "|             DELV|               687330|\n",
      "|              SDN|               438191|\n",
      "+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2(a) Top 5 vehicle body type that gets ticket.\n",
    "print ('Top 5 vehicle body type that gets ticket are :')\n",
    "df_grouped_VehicleBodyType = df.groupby(\"Vehicle Body Type\").agg({'Summons Number' : 'count'})\n",
    "df_grouped_VehicleBodyType.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Vehicle make, that gets ticket are :\n",
      "+------------+---------------------+\n",
      "|Vehicle Make|count(Summons Number)|\n",
      "+------------+---------------------+\n",
      "|        FORD|              1280958|\n",
      "|       TOYOT|              1211451|\n",
      "|       HONDA|              1079238|\n",
      "|       NISSA|               918590|\n",
      "|       CHEVR|               714655|\n",
      "+------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2(a) Top 5 Vehicle make, that gets ticket.\n",
    "print ('Top 5 Vehicle make, that gets ticket are :')\n",
    "df_grouped_vehiclemake = df.groupby(\"Vehicle Make\").agg({'Summons Number' : 'count'})\n",
    "df_grouped_vehiclemake.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Violation Precinct in terms of tickets :\n",
      "+------------------+---------------------+\n",
      "|Violation Precinct|count(Summons Number)|\n",
      "+------------------+---------------------+\n",
      "|                 0|              2072400|\n",
      "|                19|               535671|\n",
      "|                14|               352450|\n",
      "|                 1|               331810|\n",
      "|                18|               306920|\n",
      "+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Ignoring Violation Precinct 0 (Error), fetching top 6:\n",
      "+------------------+---------------------+\n",
      "|Violation Precinct|count(Summons Number)|\n",
      "+------------------+---------------------+\n",
      "|                 0|              2072400|\n",
      "|                19|               535671|\n",
      "|                14|               352450|\n",
      "|                 1|               331810|\n",
      "|                18|               306920|\n",
      "|               114|               296514|\n",
      "+------------------+---------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3(a) Top 5 Violation Precinct in terms of tickets.\n",
    "print ('Top 5 Violation Precinct in terms of tickets :')\n",
    "df_grouped_ViolationPrecinct = df.groupby(\"Violation Precinct\").agg({'Summons Number' : 'count'})\n",
    "df_grouped_ViolationPrecinct.sort(['count(Summons Number)'], ascending = False).show(5)\n",
    "print('Ignoring Violation Precinct 0 (Error), fetching top 6:')\n",
    "df_grouped_ViolationPrecinct.sort(['count(Summons Number)'], ascending = False).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Issuer Precinct in terms of tickets :\n",
      "+---------------+---------------------+\n",
      "|Issuer Precinct|count(Summons Number)|\n",
      "+---------------+---------------------+\n",
      "|              0|              2388479|\n",
      "|             19|               521513|\n",
      "|             14|               344977|\n",
      "|              1|               321170|\n",
      "|             18|               296553|\n",
      "+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Ignoring Violation Precinct 0 (Error), fetching top 6:\n",
      "+---------------+---------------------+\n",
      "|Issuer Precinct|count(Summons Number)|\n",
      "+---------------+---------------------+\n",
      "|              0|              2388479|\n",
      "|             19|               521513|\n",
      "|             14|               344977|\n",
      "|              1|               321170|\n",
      "|             18|               296553|\n",
      "|            114|               289950|\n",
      "+---------------+---------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3(b) Top 5 Issuer Precinct in terms of tickets.\n",
    "print ('Top 5 Issuer Precinct in terms of tickets :')\n",
    "df_grouped_IssuerPrecinct = df.groupby(\"Issuer Precinct\").agg({'Summons Number' : 'count'})\n",
    "df_grouped_IssuerPrecinct.sort(['count(Summons Number)'], ascending = False).show(5)\n",
    "print('Ignoring Violation Precinct 0 (Error), fetching top 6:')\n",
    "df_grouped_IssuerPrecinct.sort(['count(Summons Number)'], ascending = False).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. violation code frequencies for three precincts that have issued the most number of tickets( 19, 14 , 1).\n",
    "\n",
    "df_filtered_19 = df.filter((col('Violation Precinct').cast('Int') == 19))\n",
    "df_filtered_19_grouped = df_filtered_19.groupby('Violation Code').agg({'Summons Number' : 'count'})\n",
    "\n",
    "df_filtered_14 = df.filter((col('Violation Precinct').cast('Int') == 14))\n",
    "df_filtered_14_grouped = df_filtered_14.groupby('Violation Code').agg({'Summons Number' : 'count'})\n",
    "\n",
    "df_filtered_1 = df.filter((col('Violation Precinct').cast('Int') == 1))\n",
    "df_filtered_1_grouped = df_filtered_1.groupby('Violation Code').agg({'Summons Number' : 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Precinct 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of different voilation code for Violation Precinct 19 :\n",
      "+--------------+---------------------+\n",
      "|Violation Code|count(Summons Number)|\n",
      "+--------------+---------------------+\n",
      "|            46|                90530|\n",
      "|            38|                74926|\n",
      "|            37|                73359|\n",
      "|            14|                58640|\n",
      "|            21|                56516|\n",
      "+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Frequency of different voilation code for Violation Precinct 19 :')\n",
    "df_filtered_19_grouped.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Precinct 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of different voilation code for Violation Precinct 14 :\n",
      "+--------------+---------------------+\n",
      "|Violation Code|count(Summons Number)|\n",
      "+--------------+---------------------+\n",
      "|            14|                75850|\n",
      "|            69|                58032|\n",
      "|            31|                40150|\n",
      "|            47|                31201|\n",
      "|            42|                20666|\n",
      "+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Frequency of different voilation code for Violation Precinct 14 :')\n",
    "df_filtered_14_grouped.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Precinct 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of different voilation code for Violation Precinct 1 :\n",
      "+--------------+---------------------+\n",
      "|Violation Code|count(Summons Number)|\n",
      "+--------------+---------------------+\n",
      "|            14|                76375|\n",
      "|            16|                39197|\n",
      "|            20|                28768|\n",
      "|            46|                23954|\n",
      "|            38|                17139|\n",
      "+--------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Frequency of different voilation code for Violation Precinct 1 :')\n",
    "df_filtered_1_grouped.sort(['count(Summons Number)'], ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voilation code 14 seems to be common among all  3 precincts in terms of maximum frequency of tickets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Analysing properties of parking violations across different times of the day\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|             0|       0|                 0|         0|             0|                0|           0|                 0|              0|             0|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|             0|       0|                 0|         0|             0|                0|           0|                 0|              0|             0|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Violation Time|\n",
      "+--------------+\n",
      "|         0143A|\n",
      "|         0400P|\n",
      "|         0233P|\n",
      "|         1120A|\n",
      "|         0555P|\n",
      "|         0852P|\n",
      "|         0215A|\n",
      "|         0758A|\n",
      "|         1005A|\n",
      "|         0845A|\n",
      "|         0015A|\n",
      "|         0707A|\n",
      "|         1022A|\n",
      "|         1150A|\n",
      "|         0525A|\n",
      "|         0645P|\n",
      "|         1122A|\n",
      "|         0256P|\n",
      "|         1232A|\n",
      "|         1034A|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Violation Time')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|AM/PM|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|    A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|    P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|    P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|    A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|         0555P|    P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|         0852P|    P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03|            40|              SDN|       TOYOT|                71|             71|         0215A|    A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21|            36|               UT|         BMW|                 0|              0|         0758A|    A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21|            36|             SUBN|       DODGE|                 0|              0|         1005A|    A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05|             5|               4D|         BMW|                 0|              0|         0845A|    A|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|         0015A|    A|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27|            19|             DELV|       KENWO|                18|             18|         0707A|    A|\n",
      "|    4625926610|N102911C|                NY|2016-10-27|            36|              VAN|        FORD|                 0|              0|         1022A|    A|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30|            21|              SDN|       NISSA|                44|             44|         1150A|    A|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|         0525A|    A|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07|            71|             4DSD|       VOLKS|               120|            120|         0645P|    P|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24|             7|               TK|        FORD|                 0|              0|         1122A|    A|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|         0256P|    P|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30|            20|             SUBN|       DODGE|                17|             17|         1232A|    A|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03|            36|               4D|         BMW|                 0|              0|         1034A|    A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, length, expr, col\n",
    "from pyspark.sql.functions import regexp_extract \n",
    "\n",
    "df = df.withColumn('AM/PM', substring(col(\"Violation Time\"), -1, 1))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|AM/PM|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|          0143|    A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|          0400|    P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|          0233|    P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|          1120|    A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|          0555|    P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|          0852|    P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03|            40|              SDN|       TOYOT|                71|             71|          0215|    A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21|            36|               UT|         BMW|                 0|              0|          0758|    A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21|            36|             SUBN|       DODGE|                 0|              0|          1005|    A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05|             5|               4D|         BMW|                 0|              0|          0845|    A|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|          0015|    A|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27|            19|             DELV|       KENWO|                18|             18|          0707|    A|\n",
      "|    4625926610|N102911C|                NY|2016-10-27|            36|              VAN|        FORD|                 0|              0|          1022|    A|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30|            21|              SDN|       NISSA|                44|             44|          1150|    A|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|          0525|    A|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07|            71|             4DSD|       VOLKS|               120|            120|          0645|    P|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24|             7|               TK|        FORD|                 0|              0|          1122|    A|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|          0256|    P|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30|            20|             SUBN|       DODGE|                17|             17|          1232|    A|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03|            36|               4D|         BMW|                 0|              0|          1034|    A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Violation Time\",substring(df['Violation Time'],0, 4))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|     Violation Time|AM/PM|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|1970-01-01 01:43:00|    A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|1970-01-01 04:00:00|    P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|1970-01-01 02:33:00|    P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|1970-01-01 11:20:00|    A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|1970-01-01 05:55:00|    P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|1970-01-01 08:52:00|    P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03|            40|              SDN|       TOYOT|                71|             71|1970-01-01 02:15:00|    A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21|            36|               UT|         BMW|                 0|              0|1970-01-01 07:58:00|    A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21|            36|             SUBN|       DODGE|                 0|              0|1970-01-01 10:05:00|    A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05|             5|               4D|         BMW|                 0|              0|1970-01-01 08:45:00|    A|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|               null|    A|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27|            19|             DELV|       KENWO|                18|             18|1970-01-01 07:07:00|    A|\n",
      "|    4625926610|N102911C|                NY|2016-10-27|            36|              VAN|        FORD|                 0|              0|1970-01-01 10:22:00|    A|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30|            21|              SDN|       NISSA|                44|             44|1970-01-01 11:50:00|    A|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|1970-01-01 05:25:00|    A|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07|            71|             4DSD|       VOLKS|               120|            120|1970-01-01 06:45:00|    P|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24|             7|               TK|        FORD|                 0|              0|1970-01-01 11:22:00|    A|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|1970-01-01 02:56:00|    P|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30|            20|             SUBN|       DODGE|                17|             17|1970-01-01 00:32:00|    A|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03|            36|               4D|         BMW|                 0|              0|1970-01-01 10:34:00|    A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"Violation Time\",to_timestamp(df['Violation Time'], 'hhmm'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+-------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|     Violation Time|AM/PM|     Interval|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+-------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|1970-01-01 01:43:00|    A|   Late Night|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|1970-01-01 04:00:00|    P|         Noon|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|1970-01-01 02:33:00|    P|         Noon|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|1970-01-01 11:20:00|    A|   Late Night|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|1970-01-01 05:55:00|    P|      Evening|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|1970-01-01 08:52:00|    P|   Late Night|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03|            40|              SDN|       TOYOT|                71|             71|1970-01-01 02:15:00|    A|   Late Night|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21|            36|               UT|         BMW|                 0|              0|1970-01-01 07:58:00|    A|Early Morning|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21|            36|             SUBN|       DODGE|                 0|              0|1970-01-01 10:05:00|    A|   Late Night|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05|             5|               4D|         BMW|                 0|              0|1970-01-01 08:45:00|    A|   Late Night|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|               null|    A|   Late Night|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27|            19|             DELV|       KENWO|                18|             18|1970-01-01 07:07:00|    A|Early Morning|\n",
      "|    4625926610|N102911C|                NY|2016-10-27|            36|              VAN|        FORD|                 0|              0|1970-01-01 10:22:00|    A|   Late Night|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30|            21|              SDN|       NISSA|                44|             44|1970-01-01 11:50:00|    A|   Late Night|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|1970-01-01 05:25:00|    A|Early Morning|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07|            71|             4DSD|       VOLKS|               120|            120|1970-01-01 06:45:00|    P|      Evening|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24|             7|               TK|        FORD|                 0|              0|1970-01-01 11:22:00|    A|   Late Night|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|1970-01-01 02:56:00|    P|         Noon|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30|            20|             SUBN|       DODGE|                17|             17|1970-01-01 00:32:00|    A|   Late Night|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03|            36|               4D|         BMW|                 0|              0|1970-01-01 10:34:00|    A|   Late Night|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+-------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn(\"Interval\", \n",
    "   F.when((df['Violation Time'].between('1970-01-01 04:00:00', '1970-01-01 08:00:00')) & (df['AM/PM'] == 'A'), 'Early Morning')\n",
    "    .when((df['Violation Time'].between('1970-01-01 08:00:00', '1970-01-01 00:00:00')) & (df['AM/PM'] == 'A'), 'Morning')\n",
    "    .when((df['Violation Time'].between('1970-01-01 00:00:00', '1970-01-01 04:00:00')) & (df['AM/PM'] == 'P'), 'Noon')\n",
    "    .when((df['Violation Time'].between('1970-01-01 04:00:00', '1970-01-01 08:00:00')) & (df['AM/PM'] == 'P'), 'Evening')\n",
    "    .when((df['Violation Time'].between('1970-01-01 08:00:00', '1970-01-01 00:00:00')) & (df['AM/PM'] == 'P'), 'Night')\n",
    "    .otherwise('Late Night'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3867.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(Interval#4998, 200)\n+- *(1) HashAggregate(keys=[Interval#4998], functions=[partial_count(Summons Number#4245)], output=[Interval#4998, count#5110L])\n   +- *(1) Project [Summons Number#4245, CASE WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 04:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 08:00:00)) && (substring(Violation Time#4254, -1, 1) = A)) THEN Early Morning WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 08:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 00:00:00)) && (substring(Violation Time#4254, -1, 1) = A)) THEN Morning WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 00:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 04:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Noon WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 04:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 08:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Evening WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 08:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 00:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Night ELSE Late Night END AS Interval#4998]\n      +- *(1) FileScan csv [Summons Number#4245,Violation Time#4254] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/common_folder/nyc_parking/Parking_Violations_Issued_-_Fisca..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:string,Violation Time:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:103)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$class.buildReaderWithPartitionValues(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:165)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:312)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:310)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:330)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 41 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-02a46f61626b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Interval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Summons Number'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_grouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark2.4/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3867.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(Interval#4998, 200)\n+- *(1) HashAggregate(keys=[Interval#4998], functions=[partial_count(Summons Number#4245)], output=[Interval#4998, count#5110L])\n   +- *(1) Project [Summons Number#4245, CASE WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 04:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 08:00:00)) && (substring(Violation Time#4254, -1, 1) = A)) THEN Early Morning WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 08:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 00:00:00)) && (substring(Violation Time#4254, -1, 1) = A)) THEN Morning WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 00:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 04:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Noon WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 04:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 08:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Evening WHEN (((cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) >= 1970-01-01 08:00:00) && (cast(cast(unix_timestamp(substring(Violation Time#4254, 0, 4), hhmm, Some(UTC)) as timestamp) as string) <= 1970-01-01 00:00:00)) && (substring(Violation Time#4254, -1, 1) = P)) THEN Night ELSE Late Night END AS Interval#4998]\n      +- *(1) FileScan csv [Summons Number#4245,Violation Time#4254] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/common_folder/nyc_parking/Parking_Violations_Issued_-_Fisca..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:string,Violation Time:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:103)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$class.buildReaderWithPartitionValues(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:165)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:312)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:310)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:330)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 41 more\n"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupby('Interval').agg({'Summons Number' : 'count'})\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
